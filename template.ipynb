{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CompLabNGS2024 Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data\n",
    "Visit the [Gene Expression Omnibus (GEO) website](https://www.ncbi.nlm.nih.gov/geo/) and search for the accession number you were given. Review the information on the dataset and explore the publication that generated the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a short but inclusive description of the dataset you were given. Include name of the dataset, the publication, authors, number and condition of samples and any other relevant information to allow others to understand the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing the entire analysis, create a conda environment with all the necessary packages and their versions to run the analysis. In the cell below, write the command you used to create the environment and the command to activate it. The name of the environment should be `<first name>_<last name>`. You can use ```bash conda list``` to list all the packages and their versions in the environment.\n",
    "\n",
    "Modify the example command below to include the packages you used in your analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#module load miniconda/miniconda3-4.7.12-environmentally\n",
    "!conda create -n ngs_project \\\n",
    "    python=3.7 \\\n",
    "    pandas=1.0.3 \\\n",
    "    numpy=1.18.1 \\\n",
    "    matplotlib=3.1.3 \\\n",
    "    scikit-learn=0.22.1  \\\n",
    "    pydeseq2=1.26.0  \\\n",
    "    fastqc=0.12.1  \\\n",
    "    trimmomatic=0.39  \\\n",
    "    fastuniq=1.1\n",
    "!conda activate ngs_project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the raw fastq files\n",
    "At the bottom of the GEO page, you will find a link to the `SRA Run Selector`. Click on the link. \n",
    "\n",
    "Explain what is the SRA and SRR in gerneal and how it is used to store sequencing data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `prefetch + fasterq-dump` command from the `SRA Toolkit` to download the fastq files. Write the command you used to download the fastq files. [SRA Tools Conda installation](https://anaconda.org/bioconda/sra-tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from multiprocessing import Pool\n",
    "\n",
    "BASE_DIR = '/home/yair/ngs_project'\n",
    "srr_accessions = ['SRR25115639', 'SRR25115640', 'SRR25115641', 'SRR25115642', 'SRR25115643', 'SRR25115644', \n",
    "                  'SRR25115645', 'SRR25115646', 'SRR25115647']\n",
    "\n",
    "INPUT_FASTQ_FILES = os.path.join(BASE_DIR, 'input_fastq')\n",
    "TRIMMOMATIC_OUTPUT_DIR = os.path.join(BASE_DIR, 'trimmomatic_outputs')\n",
    "FASTUNIQ_INPUTS_FILES_DIR = os.path.join(BASE_DIR, 'fastuniq_inputs')\n",
    "FASTUNIQ_OUTPUT_DIR = os.path.join(BASE_DIR, 'fastuniq_outputs')\n",
    "\n",
    "#generate folders\n",
    "os.makedirs(INPUT_FASTQ_FILES, exist_ok=True)\n",
    "os.makedirs(TRIMMOMATIC_OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(FASTUNIQ_INPUTS_FILES_DIR, exist_ok=True)\n",
    "os.makedirs(FASTUNIQ_OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_fastq(srr_accession):\n",
    "    os.chdir(INPUT_FASTQ_FILES)\n",
    "    subprocess.run(f'prefetch {srr_accession}', shell=True)\n",
    "    subprocess.run(f'fasterq-dump {srr_accession}', shell=True)\n",
    "\n",
    "with Pool() as pool:\n",
    "    pool.map(download_fastq, srr_accessions)\n",
    "\n",
    "os.chdir(BASE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality Control\n",
    "Below include all the commands, figures and output you used and generated to perform quality control on the fastq files. You can use `fastqc`, `trimmomatic`, `multiqc` and any other tool you find necessary to give a comprehensive QC and filtering of the data.\n",
    "\n",
    "Write a brief description of the quality control process and explain the figures and tables you generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we run FastQC on 3 of the input fastq files (1 for each of the three groups).\n",
    "srr_acessions_representatives = ['SRR25115639', 'SRR25115642', 'SRR25115645']\n",
    "\n",
    "FASTQC_OUTPUTS_DIR = os.path.join(BASE_DIR, 'fastqc_outputs')\n",
    "os.makedirs(FASTQC_OUTPUTS_DIR, exist_ok=True)\n",
    "\n",
    "def run_fastqc(srr_accession):\n",
    "    srr_1_path = os.path.join(INPUT_FASTQ_FILES, f'{srr_accession}_1.fastq')\n",
    "    srr_2_path = os.path.join(INPUT_FASTQ_FILES, f'{srr_accession}_2.fastq')\n",
    "    subprocess.run(f'fastqc {srr_1_path} {srr_2_path} -o {FASTQC_OUTPUTS_DIR}', shell=True)\n",
    "\n",
    "with Pool() as pool:\n",
    "    pool.map(run_fastqc, srr_acessions_representatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We examined several of the fastqc reports and noticed 2 major issues:\n",
    "* Per base sequence content tab - the first 10 bases aren't distributed according to the rest of the read sequence. That is unexpected and therefore we will use trimmomatic HEADCROP module to cut the first 10 bases of each read.\n",
    "* Sequence Duplication Levels tab - this tab indicated that there are duplicated reads.\n",
    "\n",
    "![fastqc_per_base_sequence_content.jpg](fastqc_output/fastqc_per_base_sequence_content.jpg)\n",
    "![fastqc_duplication_levels.jpg](fastqc_output/fastqc_duplication_levels.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will run trimmomatic and fastuniq\n",
    "\n",
    "def run_trimmomatic(srr_accession):\n",
    "    srr_1_path = os.path.join(INPUT_FASTQ_FILES, f'{srr_accession}_1.fastq')\n",
    "    srr_2_path = os.path.join(INPUT_FASTQ_FILES, f'{srr_accession}_2.fastq')\n",
    "    trimmomatic_output_path = os.path.join(TRIMMOMATIC_OUTPUT_DIR, f'{srr_accession}.fastq')\n",
    "    subprocess.run(f'trimmomatic PE {srr_1_path} {srr_2_path} -baseout {trimmomatic_output_path} HEADCROP:10 MINLEN:90', shell=True)\n",
    "\n",
    "with Pool() as pool:\n",
    "    pool.map(run_trimmomatic, srr_accessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Killed\n"
     ]
    }
   ],
   "source": [
    "def run_fastuniq(srr_accession):\n",
    "    fastuniq_input_path = os.path.join(FASTUNIQ_INPUTS_FILES_DIR, f'{srr_accession}.txt')\n",
    "    with open(fastuniq_input_path, 'w') as fp:\n",
    "        fp.write(os.path.join(TRIMMOMATIC_OUTPUT_DIR, f'{srr_accession}_1P.fastq') + '\\n')\n",
    "        fp.write(os.path.join(TRIMMOMATIC_OUTPUT_DIR, f'{srr_accession}_2P.fastq'))\n",
    "    \n",
    "    fastuniq_1_output = os.path.join(FASTUNIQ_OUTPUT_DIR, f'{srr_accession}_1.fastq')\n",
    "    fastuniq_2_output = os.path.join(FASTUNIQ_OUTPUT_DIR, f'{srr_accession}_2.fastq')\n",
    "    subprocess.run(f'fastuniq -i {fastuniq_input_path} -o {fastuniq_1_output} -p {fastuniq_2_output}', shell=True)\n",
    "\n",
    "with Pool() as pool:\n",
    "    pool.map(run_fastuniq, srr_accessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# Finally we run Fastqc again on the representative fastq files\n",
    "\n",
    "CLEAN_READS_FASTQC_OUTPUTS_DIR = os.path.join(BASE_DIR, 'clean_reads_fastqc_outputs')\n",
    "os.makedirs(CLEAN_READS_FASTQC_OUTPUTS_DIR, exist_ok=True)\n",
    "\n",
    "def run_fastqc_on_clean_reads(srr_accession):\n",
    "    srr_1_path = os.path.join(FASTUNIQ_OUTPUT_DIR, f'{srr_accession}_1.fastq')\n",
    "    srr_2_path = os.path.join(FASTUNIQ_OUTPUT_DIR, f'{srr_accession}_2.fastq')\n",
    "    subprocess.run(f'fastqc {srr_1_path} {srr_2_path} -o {CLEAN_READS_FASTQC_OUTPUTS_DIR}', shell=True)\n",
    "\n",
    "with Pool() as pool:\n",
    "    pool.map(run_fastqc_on_clean_reads, srr_acessions_representatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alignment and DGE\n",
    "\n",
    "Similar to what we did in class and in the assignemnts, you will now run through the steps of mapping and conducting DGE. Include below an elaborate report of your analysis, include study desgin, example count and normalized count matrices, draw summary satistics and every other information you think is relevent to report your findings (if any)\n",
    "\n",
    "Running `STAR` on a laptop or PC is almost impossible we will use one of the more recent pseudo alighners, ['kallisto`](https://pachterlab.github.io/kallisto/manual), which will allow us to quantify transcript level information in a more efficent manner.\n",
    "\n",
    "1. Download the index files from kallisto's GitHub\n",
    "```bash\n",
    "wget https://github.com/pachterlab/kallisto-transcriptome-indices/releases/download/v1/mouse_index_standard.tar.xz\n",
    "tar -xf mouse_index_standard.tar.xz\n",
    "```\n",
    "2. Install and run `kallisto`\n",
    "```bash\n",
    "conda install -c bioconda -c conda-forge kallisto==0.50.1\n",
    "kallisto quant -i index.idx -o output -t 16 sample.fastq \n",
    "```\n",
    "\n",
    "In the `output` directory you should have `abundance.tsv`. View the file and make sure you understand it. Produce quantification to all your samples.\n",
    "We now need to sum up transcript level counts to gene level counts before proceeding to `pyDESEQ`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Load transcript to gene mapping file\n",
    "t2g_file = 't2g.txt'\n",
    "t2g_df = pd.read_csv(t2g_file, sep='\\t', usecols=[0, 2], header=None, names=['transcript_id', 'gene_id'])\n",
    "\n",
    "abundance_file = 'output/abundance.tsv'\n",
    "abundance_df = pd.read_csv(abundance_file, sep='\\t')\n",
    "\n",
    "# Merge transcript abundance with gene mapping\n",
    "merged_df = pd.merge(abundance_df, t2g_df, left_on='target_id', right_on='transcript_id', how='inner')\n",
    "\n",
    "# Sum abundances at the gene level\n",
    "gene_abundance_df = merged_df.groupby('gene_id')['tpm'].sum().reset_index()\n",
    "\n",
    "# Save gene-level abundance to a new file\n",
    "print(gene_abundance_df)\n",
    "gene_abundance_df.to_csv('gene_abundance.csv', index=False)\n",
    "```\n",
    "\n",
    "Do you understand the code? Include `print` statments and rerun until you understnad what's going on fully\n",
    "\n",
    "Do this operation to each of your sample's abundance file. Using python or a different tool merge all samples to a single file.\n",
    "\n",
    "You should now be able to take the resulting `all_samples_gene_abundance.csv` and use it in `pyDeSeq2` as we did in class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **BONUS** Gene Set Enrichment Analysis\n",
    "\n",
    "Up until this point we covered material learned in class. If you want to boost your scores attempt at learning and running GSEA analysis on your DGE results.\n",
    "\n",
    "Explain what GSEA is and what insights can we draw from it.\n",
    "\n",
    "Execute a GSEA analysis using [pyGSEA](https://gseapy.readthedocs.io/en/latest/introduction.html) and summarize your results below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "This excerise is meant to 'through' in the water and might prove challanging. Attempt to work in groups to overcome challanges. \n",
    "\n",
    "If you get stuck try to post an issue, but give as much background and explanation of your problem as possible so I could help\n",
    "\n",
    "### Good luck!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ngs_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
